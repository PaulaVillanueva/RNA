\section{Mapeo de características}

\subsection{Introducción}

Utilizando el mismo dataset de Bag of Words (BOW) del ejercicio anterior debemos construir un modelo de mapeo de características auto-organizado que clasifique automáticamente los 
documentos en un arreglo de dos dimensiones. Este problema será resuelto utilizando Kohonen.

Kohonen:
\begin{itemize}
	\item Propone un orden topológico y un modelo competitivo
	\item Función de transferencia 
				\begin{align*}
					O_{i}=f(\varepsilon, W_{i}) donde W_{i}=(w_{i1}, ..., w_{in}) \in \mathbb{R}_{n}
				\end{align*}
	
	\item PRINCIPIO DE ADAPTACIÓN: Consiste en detectar la unidad cuyos parámetros sean más parecidos a la entrada $\varepsilon$.
		\begin{itemize}
			\item Establece un orden topológico y solo se cambian los parámetros de la unidad seleccionada y los de sus vecinas.
			\item El cambio es en dirección de incrementar la sililaridad entre $W_{i}$ y $\varepsilon$.
			\item La magnitud del cambio debe garantizar estabilidad asintótica.
		\end{itemize}
		Estas tres cosas hacen que la función de densidad de probabilidades de $W_{i}$ tienda a aproximar la densidad $P(\varepsilon)$.
	\item MAPAS DE KOHONEN: Es una propuesta de implementación la cual define:
		\begin{itemize}
			\item Que la unidad seleccionada en tiempo $t$ será $c$ tal que:
				\begin{align*}
					||\varepsilon(t) - W_{c}(t)|| = min\{\varepsilon(t) - W_{i}(t)\}
				\end{align*}
			
			\item Se establece una topología de entrada a partir de establecer funciones de vecindad, esto fuerza a que los pesos no crezcan de una forma desmedida respecto de los 
			pesos de las unidades vecinas. Esto hace que los entornos sean amplios al principio pero pequeños al final, hasta limitarse solo a la unidad seleccionada.
				
			\item REGLA DE KOHONEN: Actualización de los $W_{i}$.
					\begin{align*}
					 W_{i}(t+1) &=  \begin{cases}
										W_{i}(t) + \alpha(t) [ \varepsilon(t) - W_{i}(t) ] & i \in N_{c} \\
										W_{i}(t)                                           & i \not \in N_{c}  
									\end{cases} \\
					\end{align*}
			\item $\eta(t)$ es el coeficiente de aprendizaje dinámico, decreciente en el tiempo.
				\begin{align*}
					\Delta W_{ij} = \alpha(t) [ \varepsilon(t) - W_{i}(t) ]
				\end{align*}
				\begin{align*}
					\alpha(t) = \eta \Lambda(i,c)
				\end{align*}
				\begin{align*}
					\Lambda(i,c) = \begin{cases}
										1 & i = c \\
										decrece a mayor distancia entre i y c 
									\end{cases} \\
				\end{align*}
		\end{itemize}
	\item TEOREMA DE KOHONEN: Con probabilidad 1 los $W_{i}$ se ordenan de forma ascendientes o descendientes cuando $t \to
		\infty$ si $\alpha(t) \to 0$ con suficiente lentitud.
\end{itemize}

\subsection{Modelo}
Al igual que en el ejercicio anterior, el conjunto de datos contiene 900 documentos con 856 descripciones de texto 
correspondientes a compañías Brasileñas clasificadas en nueve categorías distintas.

En primer lugar sabemos que la red a entrenar tendrá un aprendizaje no supervisado, ya que no sabemos el resultado al que
queremos llegar.

Definimos nuestro modelo como:

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.7\linewidth]{img/parte2-comparacion.jpg}
	\caption{\textbf{Figura X1}Comparación}
\end{figure}

$X$ es el vector de entrada. $X \in \mathbb{R}^{856x1}$ \\

$W$ es diccionario de salida de nuestra red, cuyas claves son pares $(i,j)$ y los significados son vectores en $\mathbb{R}^{856}$.\\

$Y$ es la matriz resultado. $Y \in \mathbb{R}^{M}$ con $M = M_{1} . M_{2}$ que es el tamaño del layout, el cual será definido más adelante.\\

\subsection{Implementación}

\subsubsection{Prepocesamiento de datos}
Al igual que el ejercicio anterior, preprocesamos el dataset centrando los datos en el 0. Calculamos la media de las frecuencias de cada palabra (posición del vector) y le restamosa cada una de ellas. De esta forma, la media de cada columna del dataset resultante es 0 y los datos quedan distribuidos alrededor del 0. En este caso, los datos no se normalizan ya que quedarían todas las dimensiones con la misma varianza y la red no podría aprender correctamente.

\subsubsection{Pseudocódigo}

Inicializamos la matriz de pesos con valores random $\in[-1/2, 1/2]$.

Aclaramos que utilizamos un 70$\%$ del dataset para entrenamiento y el 30$\%$ restante lo utilizamos para validación, esto lo hicimos lara reservarnos datos y poder ver que tan bien anda nuestra red al mostrarle nuevos ejemplos.

Para calcular la activación de una capa Kohonen ve cuánto estímulo recibe una neurona de la capa anterior y ve cuál se activa. Para saber cual se activa debemos establecer un criterio de activación. Elegimos a la unidad ganadora como el $j$ que más se parece a mi vector de entrada y con esto sabemos que es la neurona que se encuentra a menor distancia. Nosotros elegimos utilizar la distancia euclidiana:

\begin{align*}
		j* = argmin_{j}||Y^{T} - W_{\bullet j}(t)||_{2}
\end{align*}

De esta forma calculamos la distancia entre $X$ y cada columna de $W$ y posteriomente nos quedamos con la neurona $j$ 
cuya distancia minimiza la función. En la \textbf{Figura X1}, mostramos cómo se realiza dicha comparación teniendo en cuenta las dimensiones.

Con esto logramos que una salida se apodere de un sector de la entrada y en consecuencia puede llegar a suceder que en nuestra red
no haya un orden topológico. Por lo tanto, una vez elegida la neurona ganadora debemos implementar una topología de entrada.
La topología que utilizamos se muestra en la \textbf{Figura X2}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.3\linewidth]{img/parte2-vecindario.jpg}
	\caption{\textbf{Figura X2}: Función de vecindario.}
\end{figure}

Esta topología esta definida como la distacia euclidiana. 

%\begin{algorithm}[H]
%	\caption{def ddistance(self, i, j):}
%	\begin{algorithmic}
%		\State $w = self._output_layout[0]$
%		\State $h = self._output_layout[1]$
%		\State $min_x = min(i[0], j[0])$
%		\State $min_y = min(i[1], j[1])$
%		\State $max_x = max(i[0], j[0])$
%		\State $max_y = max(i[1], j[1])$
%
%		\State $dist_x = min(max_x-min_x, min_x + w - max_x)$
%		\State $dist_y = min(max_y - min_y, min_y + h - max_y)$
%
%		\State return $math.sqrt(dist_x**2 + dist_y**2)$
%	\end{algorithmic}
%\end{algorithm}

Este algoritmo tiene la característica particular de ser toroidal lo cual la superficie de revolución generada por el plano gire alrededor de un eje tal como se muestra en la \textbf{Figura X3}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.5\linewidth]{img/parte2-toroidal.jpg}
	\caption{\textbf{Figura X3}: Distancia euclidiana toroidal}
\end{figure}

En la figura anterior se puede ver que dada una neurona ganadora $c$, se definen $N_{c1}$, $N_{c2}$, $N_{c3}$, $N_{c4}$ como las neuronas vecinas,siedo , $N_{c1}$ las vecinas inmediatas y , $N_{c4}$ las vecinas más lejanas a $c$. Este tipo de topología fuerza a que los pesos no crezcan en forma desmedida con respecto a las neuronas vecinas. Dichos pesos los actualizamos mediante la Regla de Kohonen:

\begin{align*}
	W_{i}(t+1) &=  \begin{cases}
						W_{i}(t) + \alpha(t) [ \varepsilon(t) - W_{i}(t) ] & i \in N_{c} \\
						W_{i}(t)                                           & i \not \in N_{c}  
					\end{cases} \\
\end{align*}

Inicialmente el vecindario está definido como todas las neuronas del plano, pero con el correr del tiempo 
este vecindario disminuye su tamaño hasta afectar solamente el peso de la neurona ganadora. Esto lo regulamos con la variable $tau$. Si inicialmente, $tau$ es muy chico



Para colorear el layout utilizamos el siguiente criterio: A cada categoría le asignamos un color. Para cada unidad de entrada contamos los inputs  que están asociados a la misma categoría y de ellos elegimos la categoría que corresponda a la mayoría. Uno de los problemas que no solucionamos fue cuando los porcentajes de categorías son similares. Por ejemplo: Dadas las categorías A, B y C. Si a una neurona le corresponde en un 40$\%$ la Categoría A, 40$\%$ la Categoría B y 20$\%$ la Categoría C nuestro algoritmo no es del todo confiable.

El layout es un plano en el que coloreamos las coordenadas del color asociado a la categoría de la neurona ganadora. Este puede tener distintas dimensiones según lo definamos. Nosotros elegimos que nuestro layout tenga dimensión 15 $x$ 15 y un ejemplo de coloreo es la \textbf{Figura X4}.

%\begin{figure}[ht!]
%	\centering
%	\includegraphics[width=0.7\linewidth]{img/parte2-layout.jpg}
%	\caption{\textbf{Figura X4}: Ejemplo de Layout de dimensión $15 x 15$.}
%\end{figure}

En cuanto al clasificador del conjunto de test, para un X dado buscamos la unidad que más se parece a $X$ en $W$ y asignamos la categoría de la misma, como señala la \textbf{Figura X1}. En este clasificador no consideramos las unidades a las que el training no les asignó categoría.

\subsection{Ejecución}

\subsubsection{Modo de uso}

\textbf{Para entrenar la red:}

\begin{lstlisting}[style=bash]
usage: khtrain.py [-h] -p P -x X -w W -l L -n N

Parametros de la red

optional arguments:

  -h, --help  show this help message and exit

  -p P        Ruta del archivo de salida para escribir pesos

  -x X        Archivo de training features

  -w W        Ancho del arreglo de neuronas de salida

  -l L        Alto del arreglo de neuronas de salida

  -n N        Cantidad de epochs a entrenar
\end{lstlisting}

Ejemplo: 

\noindent\texttt{python khtrain.py -h} \\

\textbf{Para mostrar el mapa coloreado según las categorías:}

\begin{lstlisting}[style=bash]
usage: khdispplaymap.py [-h] -p P

Parametros

optional arguments:

  -h, --help  show this help message and exit

  -p P        Ruta del archivo de entrada con los pesos para mostrar el mapa

\end{lstlisting}

Ejemplo:

\noindent\texttt{python khdispplaymap.py -h} \\


\textbf{Para el clasificador:}

\begin{lstlisting}[style=bash]
usage: khclassify.py [-h] -p P -x X

Parametros de la red

optional arguments:

  -h, --help  show this help message and exit

  -p P        Ruta del archivo de entrada con los pesos

  -x X        Archivo de features
\end{lstlisting}

Ejemplo:

\noindent\texttt{python khclassify.py -h} \\


Para que un modelo sea auto-organizado debemos elegir el sigma $\sigma$ y  el eta $\eta$ adecuados. El sigma será 
utilizado como parámetro de la Gauseana y medirá la dispersión, cuantas neuronas vecinas a la neurona activada modificarán 
sus pesos. Por otro lado, el $\eta$ será el coeficiente de aprendizaje de nuestra red.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.8\linewidth]{img/parte2-kohonen9clases.jpg}
	\caption{\textbf{Figura X4}Clasificación con Kohonen}
\end{figure}


\subsection{Resultados}


\subsection{Conclusiones}

